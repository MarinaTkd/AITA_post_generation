{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19b65b1",
      "metadata": {
        "id": "f19b65b1"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# Extract SQLite file from ZIP archive\n",
        "with zipfile.ZipFile(\"AmItheAsshole.sqlite.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extract(\"AmItheAsshole.sqlite\")\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect(\"AmItheAsshole.sqlite\")\n",
        "\n",
        "# Retrieve table names\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Print the list of tables\n",
        "print(\"Tables in the SQLite database:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Close the connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc03ab7c",
      "metadata": {
        "id": "cc03ab7c"
      },
      "outputs": [],
      "source": [
        "# Brining in posts\n",
        "conn = sqlite3.connect(\"AmItheAsshole.sqlite\")\n",
        "\n",
        "query = \"SELECT * FROM submission\"\n",
        "\n",
        "if conn is not None:\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "else:\n",
        "    print(\"Error: Database connection is closed.\")\n",
        "\n",
        "df.to_csv('posts.csv')\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28a9176",
      "metadata": {
        "id": "d28a9176"
      },
      "outputs": [],
      "source": [
        "# Brining in comments\n",
        "conn = sqlite3.connect(\"AmItheAsshole.sqlite\")\n",
        "\n",
        "query = \"SELECT * FROM comment\"\n",
        "\n",
        "if conn is not None:\n",
        "    df_comment = pd.read_sql_query(query, conn)\n",
        "    df_comment.to_csv('comments.csv')\n",
        "else:\n",
        "    print(\"Error: Database connection is closed.\")\n",
        "\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bfb3aa2",
      "metadata": {
        "id": "0bfb3aa2"
      },
      "outputs": [],
      "source": [
        "df_posts = pd.read_csv('posts.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ef2dc6",
      "metadata": {
        "id": "15ef2dc6"
      },
      "outputs": [],
      "source": [
        "df_comment = pd.read_csv('comments.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338ead38",
      "metadata": {
        "id": "338ead38"
      },
      "source": [
        "## Reviwing existing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06cc78b4",
      "metadata": {
        "id": "06cc78b4"
      },
      "outputs": [],
      "source": [
        "# Count duplicates in each column\n",
        "def count_duplicates_in_columns(df):\n",
        "    duplicate_counts = {}\n",
        "    for column in df.columns:\n",
        "        duplicates_count = df[df.duplicated(subset=[column], keep=False)].shape[0]\n",
        "        duplicate_counts[column] = duplicates_count\n",
        "    return duplicate_counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5e2f43",
      "metadata": {
        "id": "2b5e2f43"
      },
      "source": [
        "### Posts df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f677e25",
      "metadata": {
        "id": "0f677e25"
      },
      "outputs": [],
      "source": [
        "df_posts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5fc847",
      "metadata": {
        "id": "5a5fc847"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# Check the data types and non-null counts\n",
        "display(df_posts.info())\n",
        "\n",
        "\n",
        "# Get a summary of the categorical columns\n",
        "display(df_posts['title'].describe())\n",
        "display(df_posts['selftext'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7b5d6",
      "metadata": {
        "id": "0be7b5d6"
      },
      "outputs": [],
      "source": [
        "# Rename 'Unnamed: 0' to 'index'\n",
        "df_posts.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\n",
        "\n",
        "# Drop duplicate rows if any\n",
        "df_posts['selftext'].drop_duplicates(inplace=True)\n",
        "\n",
        "df_posts['selftext'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d0c2575",
      "metadata": {
        "id": "0d0c2575"
      },
      "source": [
        "\n",
        "Analysis of TITLE colum indicates duplicate in the title, which doesn't necceserraly mean that it is a duplicate record, as titles can overlap.\n",
        "Analysis of the post body indicates we have quite a few removed posts and some duplicates, both will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec69b88b",
      "metadata": {
        "id": "ec69b88b"
      },
      "outputs": [],
      "source": [
        "# Removing empty posts and duplicats\n",
        "# Filter out rows where the 'selftext' column is '[removed]'\n",
        "df_posts = df_posts[df_posts['selftext'] != '[removed]']\n",
        "\n",
        "# Removing duplicate entries in the 'selftext' column\n",
        "df_posts.drop_duplicates(subset='selftext', inplace=True)\n",
        "\n",
        "# Renaming the 'selftext' column to 'body'\n",
        "df_posts.rename(columns={'selftext': 'body'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0c1e0d",
      "metadata": {
        "id": "ec0c1e0d"
      },
      "outputs": [],
      "source": [
        "# reviewing the title duplicates to understand weather it's just and accidentally same selected title for different posts:\n",
        "import random\n",
        "\n",
        "duplicated_titles = df_posts[df_posts.duplicated(subset=['title'], keep=False)]['title'].unique()\n",
        "random_title = random.choice(duplicated_titles)\n",
        "records_with_title = df_posts[df_posts['title'] == random_title]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c37896",
      "metadata": {
        "id": "53c37896"
      },
      "outputs": [],
      "source": [
        "records_with_title['body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4484beda",
      "metadata": {
        "id": "4484beda"
      },
      "outputs": [],
      "source": [
        "rows_to_display = [1307, 1370]\n",
        "\n",
        "for index in rows_to_display:\n",
        "    print(records_with_title.loc[index, 'body'])\n",
        "    print()  # Add a newline for clarity between rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb05c461",
      "metadata": {
        "id": "bb05c461"
      },
      "source": [
        "so it seems that the posts with the same title are very likely a duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23567240",
      "metadata": {
        "id": "23567240"
      },
      "outputs": [],
      "source": [
        "#reviewed another example and deleted to not cluter this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17dea5d8",
      "metadata": {
        "id": "17dea5d8"
      },
      "source": [
        "Interestingly, the story iis not a word by word copy, but it is clearly the same story. For clenliness we will drop all the rows that have duplcate title."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ac226d",
      "metadata": {
        "id": "87ac226d"
      },
      "source": [
        "We have droped all suplicates in the body and explored remaining duplicates in the title. Those are also duplicates but usually with some extra sentance, for instance \"Reposting from different account\". Therefore we droped all the title duplicates as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e94829",
      "metadata": {
        "id": "36e94829"
      },
      "outputs": [],
      "source": [
        "df_posts.drop_duplicates(subset='title', keep='first', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcfe18b3",
      "metadata": {
        "id": "bcfe18b3"
      },
      "outputs": [],
      "source": [
        "# Count duplicates in each column\n",
        "duplicate_counts = count_duplicates_in_columns(df_posts)\n",
        "duplicate_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af83d75c",
      "metadata": {
        "id": "af83d75c"
      },
      "outputs": [],
      "source": [
        "# Drop columns 'Unnamed: 0', 'id', and 'created_utc'\n",
        "columns_to_drop = ['Unnamed: 0', 'id', 'created_utc']\n",
        "df_posts.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Set 'submission_id' as the index\n",
        "df_posts.set_index('submission_id', inplace=True)\n",
        "\n",
        "# Print the first few rows to verify changes\n",
        "df_posts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be77947",
      "metadata": {
        "id": "8be77947"
      },
      "source": [
        "Following the same best practices as in other AITA reaserch I'm dropping all posts that have score under 3 (under 3 number of upvotes), to ensure good quality of posts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b395bf07",
      "metadata": {
        "id": "b395bf07"
      },
      "outputs": [],
      "source": [
        "# Drop rows where 'score' is under 3\n",
        "df_posts = df_posts[df_posts['score'] >= 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c1569e",
      "metadata": {
        "id": "d9c1569e"
      },
      "outputs": [],
      "source": [
        "df_posts['score'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508ead90",
      "metadata": {
        "id": "508ead90"
      },
      "source": [
        "### In depth body analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53619f9a",
      "metadata": {
        "id": "53619f9a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f8512d",
      "metadata": {
        "id": "14f8512d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73fae6ac",
      "metadata": {
        "id": "73fae6ac"
      },
      "outputs": [],
      "source": [
        "# Converting to lowercase\n",
        "df_posts['cleaned_body'] = df_posts['body'].str.lower()\n",
        "\n",
        "# Removing \"AITA\"\n",
        "df_posts['cleaned_body'] = df_posts['cleaned_body'].str.replace(r'\\baita\\b', '')\n",
        "\n",
        "# Removing special characters, numbers, etc. (if needed)\n",
        "df_posts['cleaned_body'] = df_posts['cleaned_body'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "# Removing stopwords (example)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_posts['cleaned_body'] = df_posts['cleaned_body'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22dc53e",
      "metadata": {
        "id": "b22dc53e"
      },
      "outputs": [],
      "source": [
        "df_posts['word_count'] = df_posts['cleaned_body'].str.split().str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af25e97",
      "metadata": {
        "id": "1af25e97"
      },
      "outputs": [],
      "source": [
        "df_posts['word_count'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a1fbb7",
      "metadata": {
        "id": "26a1fbb7"
      },
      "outputs": [],
      "source": [
        "# Droping posts with word count less than 50\n",
        "df_posts = df_posts[df_posts['word_count'] >= 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734fb2d3",
      "metadata": {
        "id": "734fb2d3"
      },
      "outputs": [],
      "source": [
        "# Drop the first row which only contains subreddit rules\n",
        "df_posts = df_posts.drop(df_posts.index[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcec9c2",
      "metadata": {
        "id": "0bcec9c2"
      },
      "outputs": [],
      "source": [
        "len(df_posts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa419cec",
      "metadata": {
        "id": "aa419cec"
      },
      "source": [
        "We removed duplicates by both body and title columns, droped all rows with score lower then 3 and with post word_cound under 50 to ensure decent quality of stories. We also removed fist record as it only contains rules of the subreaddit instaed of the post.\n",
        "\n",
        "We have completed coule of preprocessing steps, lowecased everything, removed special characters and aita keyword.\n",
        "\n",
        "Those changes have left us with the 30135 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf039a3f",
      "metadata": {
        "id": "bf039a3f"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to CSV\n",
        "df_posts.to_csv('df_posts_cleaned.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4ed066",
      "metadata": {
        "id": "6e4ed066"
      },
      "outputs": [],
      "source": [
        "df_posts_cleaned = pd.read_csv(\"df_posts_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8454e100",
      "metadata": {
        "id": "8454e100"
      },
      "outputs": [],
      "source": [
        "post_ids = df_posts_cleaned['submission_id'].unique()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}