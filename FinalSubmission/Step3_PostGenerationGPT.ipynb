{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YeZNRmc9KIuW",
      "metadata": {
        "id": "YeZNRmc9KIuW"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XlvqTgVyKb44",
      "metadata": {
        "id": "XlvqTgVyKb44"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from pprint import pprint\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "import time\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b92b0e2-a8e1-4fb1-808e-9f3bbafa60bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b92b0e2-a8e1-4fb1-808e-9f3bbafa60bb",
        "outputId": "81cf415a-47d8-4221-c20e-7c240cd8fb44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = ''\n",
        "login(token= hf_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddfad43-ec7d-401e-97cb-ad512401140d",
      "metadata": {
        "id": "7ddfad43-ec7d-401e-97cb-ad512401140d"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T171laWKpT2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "T171laWKpT2S",
        "outputId": "7fa0c32f-efdc-4f06-f54d-8eee53aee7e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-250e1b8e-0dcc-43df-a8f9-4a37708b2002\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-250e1b8e-0dcc-43df-a8f9-4a37708b2002\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AITA_minorities_subset.csv to AITA_minorities_subset.csv\n",
            "Saving aita_subset_titles.csv to aita_subset_titles.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b4fdad-abd9-49e6-bdcc-35e152c5cb66",
      "metadata": {
        "id": "a1b4fdad-abd9-49e6-bdcc-35e152c5cb66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "aita_subset = pd.read_csv('AITA_minorities_subset.csv')\n",
        "\n",
        "train_data = aita_subset[101:200]\n",
        "test_data = pd.read_csv('aita_subset_titles.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cMyg2s6idIoG",
      "metadata": {
        "id": "cMyg2s6idIoG"
      },
      "source": [
        "# STEP 3 Post Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8w949z8TwFiR",
      "metadata": {
        "id": "8w949z8TwFiR"
      },
      "outputs": [],
      "source": [
        "# this length is selected to accomodate the average length aita post\n",
        "post_max_tokens = 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3563a427-3fe8-45c1-9214-f88b0ade1a32",
      "metadata": {
        "id": "3563a427-3fe8-45c1-9214-f88b0ade1a32"
      },
      "outputs": [],
      "source": [
        "post_gen_system_message = f\"\"\"\n",
        "You are a highly intelligent language model designed to generate posts for the \"Am I the Asshole?\" (AITA) subreddit. Your task is to read the title of an AITA post and generate an engaging post that aligns with the provided judgement.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Read the Title:\n",
        "Carefully read the title of the AITA post.\n",
        "\n",
        "2. Acknowledge the Required Judgement:\n",
        "Consider the judgement indicated (NTA - Not the Asshole, NAH - No Assholes Here, ESH - Everyone Sucks Here). Think about the key points you need to make to ensure the story aligns with the required judgement.\n",
        "\n",
        "3. Generate a Story by Following those Steps:\n",
        "\n",
        "Create an AITA post without using section titles. Include the following elements seamlessly in the narrative:\n",
        "\n",
        "Brief Background: Provide context for the story. Introduce the main characters and their relationships, and set the scene for the events that will unfold.\n",
        "\n",
        "The Incident: Describe the specific incident or series of events that led to the conflict. Be detailed and clear about what happened, who was involved, and what actions were taken.\n",
        "\n",
        "Friends' or Family's Opinion: Describe the opinions of friends or family members about the situation. Include differing viewpoints to provide a balanced perspective on the conflict.\n",
        "\n",
        "The Current Outcome: Detail the consequences of the incident. Explain how the people involved reacted, any changes in relationships, and any ongoing impact the conflict has had.\n",
        "\n",
        "Conclusion:\n",
        "Summarize the key points of the story and pose similar question to the readers: \"Am I the Asshole for [OP ACTIONS]?\"\n",
        "\n",
        "4. Maintain Authenticity:\n",
        "Ensure that the story feels realistic and relatable. Use natural language and tone as if a real person is sharing their experience.\n",
        "\n",
        "5. Adhere to the Judgement:\n",
        "Ensure that the generated story logically leads to the required judgment (e.g., if the judgment is NTA, the story should clearly indicate why the poster might be considered not the asshole).\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restricting length to avoid using lengthy posts into the prompt hence to many tokens\n",
        "# but also avoiding passing too short example to not prompt the model for short story generation\n",
        "percentile_75 = int(train_data['word_count'].quantile(0.75))\n",
        "percentile_25 = int(train_data['word_count'].quantile(0.25))\n",
        "train_data= train_data[train_data['word_count'] < percentile_75]\n",
        "train_data= train_data[train_data['word_count'] > percentile_25]\n",
        "train_data = train_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "aWPlDqBMID1f"
      },
      "id": "aWPlDqBMID1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tHMEzd6NrOM8",
      "metadata": {
        "id": "tHMEzd6NrOM8"
      },
      "outputs": [],
      "source": [
        "train_data['few_shot_input'] = 'Judgement: ' + train_data['label'] + ', Title: ' + train_data['title']\n",
        "train_data['few_shot_output'] = train_data['body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M6JU24CKrK4B",
      "metadata": {
        "id": "M6JU24CKrK4B"
      },
      "outputs": [],
      "source": [
        "def create_example(row):\n",
        "  one_shot_data = row\n",
        "  one_shot = []\n",
        "  for os_index, os_row in one_shot_data.iterrows():\n",
        "    one_shot.append({\"role\": \"user\", \"content\": os_row['few_shot_input']})\n",
        "    one_shot.append({\"role\": \"assistant\", \"content\": os_row['few_shot_output']})\n",
        "\n",
        "  return one_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZHwOLWgMsWZI",
      "metadata": {
        "id": "ZHwOLWgMsWZI"
      },
      "outputs": [],
      "source": [
        "test_data['gen_post_prompt'] = \"Judgement: \" + test_data['label'] + \", Title: \" + test_data['generated_titles']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I5DC5AHcsIi7",
      "metadata": {
        "id": "I5DC5AHcsIi7"
      },
      "outputs": [],
      "source": [
        "def format_post_gen_input(row):\n",
        "  if row['label'] == 'YTA':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'YTA'].sample(1))\n",
        "  elif row['label'] == 'NAH':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'NAH'].sample(1))\n",
        "  elif row['label'] == 'ESH':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'ESH'].sample(1))\n",
        "\n",
        "  system_message = [{\"role\": \"system\", \"content\": post_gen_system_message}]\n",
        "  user_message = [{\"role\": \"user\", \"content\": row['gen_post_prompt']}]\n",
        "  return system_message + one_shot + user_message\n",
        "\n",
        "\n",
        "test_data.loc[:, 'get_post_input'] = test_data.apply(format_post_gen_input, axis=1)\n",
        "#pprint(test_data.loc[:1, 'get_post_input'].tolist(), sort_dicts=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Sy0ZkToyjan",
      "metadata": {
        "id": "9Sy0ZkToyjan"
      },
      "source": [
        "## Clearing GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yFvNrb8R0EA6",
      "metadata": {
        "id": "yFvNrb8R0EA6"
      },
      "source": [
        "## Generating with GPT 4o mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3DP1TsVw9S5u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DP1TsVw9S5u",
        "outputId": "cd230a68-012d-400f-94b7-056079613931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.40.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KUQkCAu69PMd",
      "metadata": {
        "id": "KUQkCAu69PMd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SoEjvndzyOGD",
      "metadata": {
        "id": "SoEjvndzyOGD"
      },
      "outputs": [],
      "source": [
        "def generate_posts(formated_input):\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.9,\n",
        "    max_tokens = post_max_tokens,\n",
        "    messages=formated_input,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = test_data['get_post_input'][0]"
      ],
      "metadata": {
        "id": "nmnolzp0fE1N"
      },
      "id": "nmnolzp0fE1N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "chat_completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  temperature=0.9,\n",
        "  max_tokens = post_max_tokens,\n",
        "  messages=test)"
      ],
      "metadata": {
        "id": "6D1xTpONfV7O"
      },
      "id": "6D1xTpONfV7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLorehz3gAPS",
        "outputId": "2ce9f9b7-e938-45a7-9a93-67590ef65161"
      },
      "id": "pLorehz3gAPS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9wWCZQSws5v6DhbE6W4UOUuL09vyz',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"I’ve been friends with Jake for over ten years. He’s always been a great friend, but he has a somewhat mischievous sense of humor and a penchant for collecting what he calls “memorable moments.” Not too long ago, he decided to hang up a large photo collage in his living room, and one of the pictures he included is of the two of us at a party years ago. In that photo, we’re both having a great time, but my wife is convinced I was being a bit too friendly with an ex of mine who’s also in the picture. \\n\\nHonestly, it was just a fun night out—everyone was having a blast, and it didn’t mean anything. But my wife has been having anxiety about it since we got married. She’s expressed her discomfort with that photo being on display, saying it feels like a reminder of my past relationships. At first, I brushed it off thinking it was just her being overly sensitive. But as time went on, I couldn’t help but notice how it affected her mood every time we visited Jake. \\n\\nDuring a recent trip to Jake’s, I finally talked to him about it. I asked if he could take that specific photo down to help ease my wife's discomfort. To my surprise, Jake reacted negatively and called me unreasonable. He said he loved that photo and didn’t want to change his home for “one person’s insecurity.” This made me feel kind of crappy because I really thought I was just looking out for my wife’s feelings.\\n\\nNow, our mutual friends are split on the issue. Some believe that Jake should respect my wife’s feelings, while others think I overstepped and should have just let it go. I started feeling guilty and second-guessing myself. I genuinely didn’t mean to offend Jake, but I also want to prioritize my wife’s comfort. \\n\\nJake is still upset with me, and I’m not sure how to resolve it. I know I might have crossed a line, but I just wanted my wife to feel secure. AITA for expecting my friend to remove a sentimental photo that's making my wife uncomfortable?\",\n",
              "    'refusal': None,\n",
              "    'role': 'assistant'}}],\n",
              " 'created': 1723733835,\n",
              " 'model': 'gpt-4o-mini-2024-07-18',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': 'fp_48196bc67a',\n",
              " 'usage': {'completion_tokens': 428,\n",
              "  'prompt_tokens': 878,\n",
              "  'total_tokens': 1306}}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data['generated_posts_gpt_mini'] = test_data['get_post_input'].apply(generate_posts)\n"
      ],
      "metadata": {
        "id": "qh-uLH4ReCqW"
      },
      "id": "qh-uLH4ReCqW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv('aita_subset_gpt_mini_posts.csv')"
      ],
      "metadata": {
        "id": "aN8ZScy3i3mF"
      },
      "id": "aN8ZScy3i3mF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wrwIFgjgeuE",
        "outputId": "16fbce5b-82a8-40de-911c-ce507f5634d3"
      },
      "id": "2wrwIFgjgeuE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’ve been friends with Jake for over ten years. He’s always been a great friend, but he has a somewhat mischievous sense of humor and a penchant for collecting what he calls “memorable moments.” Not too long ago, he decided to hang up a large photo collage in his living room, and one of the pictures he included is of the two of us at a party years ago. In that photo, we’re both having a great time, but my wife is convinced I was being a bit too friendly with an ex of mine who’s also in the picture. \n",
            "\n",
            "Honestly, it was just a fun night out—everyone was having a blast, and it didn’t mean anything. But my wife has been having anxiety about it since we got married. She’s expressed her discomfort with that photo being on display, saying it feels like a reminder of my past relationships. At first, I brushed it off thinking it was just her being overly sensitive. But as time went on, I couldn’t help but notice how it affected her mood every time we visited Jake. \n",
            "\n",
            "During a recent trip to Jake’s, I finally talked to him about it. I asked if he could take that specific photo down to help ease my wife's discomfort. To my surprise, Jake reacted negatively and called me unreasonable. He said he loved that photo and didn’t want to change his home for “one person’s insecurity.” This made me feel kind of crappy because I really thought I was just looking out for my wife’s feelings.\n",
            "\n",
            "Now, our mutual friends are split on the issue. Some believe that Jake should respect my wife’s feelings, while others think I overstepped and should have just let it go. I started feeling guilty and second-guessing myself. I genuinely didn’t mean to offend Jake, but I also want to prioritize my wife’s comfort. \n",
            "\n",
            "Jake is still upset with me, and I’m not sure how to resolve it. I know I might have crossed a line, but I just wanted my wife to feel secure. AITA for expecting my friend to remove a sentimental photo that's making my wife uncomfortable?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['generated_posts_gpt_mini'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "T-0-FkRbi7rY",
        "outputId": "e12bed56-bb0b-4265-c3d7-fe76e229b132"
      },
      "id": "T-0-FkRbi7rY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I (30M) have a close friend, Jake (32M), who recently moved in with his girlfriend. They've been dating for a few years, and while we get along well, he has a tendency to display photos and memorabilia from past relationships. When they settled into their new place, he put up a large, sentimental photo of him with his ex-girlfriend at a festival. He’s mentioned how much that photo means to him, but it has become a point of tension for my wife (28F).\\n\\nThe issue came to a head when we visited Jake’s new place for the first time. As soon as we walked in, my wife’s face dropped when she saw the picture hanging prominently in the living room. She tried to play it cool, but I could see it was eating at her. Later, she confided in me that seeing the photo made her uncomfortable. She felt disrespected and worried that I might inadvertently compare her to Jake’s ex.\\n\\nI totally understand the sentiment behind the photo, but I also want my wife to feel comfortable in any situation. So, I brought it up to Jake. I asked if he would mind taking it down or moving it to a less visible spot. I explained how it made my wife feel and stressed that it wasn’t about discrediting his past but rather about making our partner feel secure.\\n\\nJake reacted pretty negatively. He called me out for trying to dictate what he can or can't do in his own home. He insisted that I was overreacting and that I was being unreasonable. He said he had every right to honor his past, and it was unfair for me to ask him to change that. The conversation ended on a sour note, and now he’s upset with me, and I feel like I’ve ruined our friendship over this.\\n\\nMy wife thinks I did the right thing, while some mutual friends are divided. Some agree with me, believing that while memories are important, they shouldn’t come at the cost of a partner’s comfort. Others think I was out of line for asking him to change his decor, suggesting that it’s a part of who he is.\\n\\nSo, AITA for expecting my friend to remove or relocate a sentimental photo that’s making my wife uncomfortable?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating with GPT 4o"
      ],
      "metadata": {
        "id": "EMhhRy3im1fY"
      },
      "id": "EMhhRy3im1fY"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_posts(formated_input):\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.9,\n",
        "    max_tokens = post_max_tokens,\n",
        "    messages=formated_input,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "2XK_TvP1m2oU"
      },
      "id": "2XK_TvP1m2oU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test_data['get_post_input'][0]"
      ],
      "metadata": {
        "id": "ghh81vw2m7F-"
      },
      "id": "ghh81vw2m7F-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "chat_completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  temperature=0.9,\n",
        "  max_tokens = post_max_tokens,\n",
        "  messages=test)"
      ],
      "metadata": {
        "id": "Wr6MZEHtnCme"
      },
      "id": "Wr6MZEHtnCme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0-BItIDnKpv",
        "outputId": "702ea3d7-ca7a-42e4-f66d-566f46b29a97"
      },
      "id": "f0-BItIDnKpv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9wWV24iXnuqBaFzJPgE6TIVTLGvm6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I’ve had a close friend, Jake, for over a decade. We’ve shared countless memories and have always been there for each other. Recently, he posted a photo on social media that was taken at a party a few years back. In it, I’m with my wife, and Jake’s arm is wrapped around her shoulder in a friendly way. The problem? My wife is uncomfortable with how she looks in the picture, and honestly, it feels a bit too intimate for our liking.\\n\\nWhen my wife first saw it, she fumbled with her words, trying to articulate why it bothered her. She mentioned that she didn’t like the way she looked and that it gave off a vibe that didn’t sit well with her. I understood her concerns – it’s important for couples to communicate boundaries and feelings, especially when it comes to representation in shared spaces like social media.\\n\\nSo, I brought it up with Jake during a casual hangout. I explained my wife's discomfort with the photo and asked if he could take it down. I thought it was a reasonable request given the context. Instead of being understanding, Jake got defensive and said that it was just a fun photo from a good time and that it didn’t mean anything. He insisted that he shouldn’t have to remove it just because my wife didn’t feel great about it, saying it was a personal photo that he liked.\\n\\nJake’s reaction surprised me. I assumed that as a good friend, he would understand where I was coming from, but his insistence made me feel unsure. I tried to explain further, but it just turned into a back-and-forth where he stated I was being overprotective. I left it at that, deciding not to push him further, but I couldn’t shake the feeling that perhaps I overstepped by even asking.\\n\\nNow, I see this affecting our friendship, and I worry that I may have been the jerk in expecting Jake to change something that he saw as innocent. My wife thinks it’s important to maintain our boundaries, but now I’m unsure if I should have considered Jake’s feelings more.\\n\\nSo, AITA for expecting my friend to remove a sentimental photo that’s making my wife uncomfortable?\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1723734980, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=443, prompt_tokens=878, total_tokens=1321))"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "test_data['generated_posts_gpt_4o'] = test_data['get_post_input'].apply(generate_posts)\n",
        "\n",
        "print(f'Time: {int(time.time() - start_time)} seconds')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZu-wtz3nSFn",
        "outputId": "fb05bd8c-51cb-4598-cc82-dce2442270e2"
      },
      "id": "DZu-wtz3nSFn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 1400 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv('aita_subset_gpt_posts.csv')"
      ],
      "metadata": {
        "id": "9t5W2D-dntGE"
      },
      "id": "9t5W2D-dntGE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('aita_subset_gpt_posts.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2OQQzMXOn1Xh",
        "outputId": "76f95875-f911-4dc9-a431-9f6081f65bec"
      },
      "id": "2OQQzMXOn1Xh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9aa9f43c-4654-4986-94a6-3f7400c46522\", \"aita_subset_gpt_posts.csv\", 1607178)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbwIuSFsxzxa"
      },
      "id": "NbwIuSFsxzxa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}