{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YeZNRmc9KIuW",
      "metadata": {
        "id": "YeZNRmc9KIuW"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2I_hw8B9KFnA",
      "metadata": {
        "id": "2I_hw8B9KFnA"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cppgpWr4KWTg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cppgpWr4KWTg",
        "outputId": "017d001f-43a3-438e-d220-3fe1abc4777c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading transformers-4.44.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cudnn-cu12, torch, transformers, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 torch-2.4.0 torchvision-0.19.0 transformers-4.44.1 triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XlvqTgVyKb44",
      "metadata": {
        "id": "XlvqTgVyKb44"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from pprint import pprint\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "import time\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b92b0e2-a8e1-4fb1-808e-9f3bbafa60bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b92b0e2-a8e1-4fb1-808e-9f3bbafa60bb",
        "outputId": "5911357e-a958-4ea4-f181-8c07e09cd68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = ''\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddfad43-ec7d-401e-97cb-ad512401140d",
      "metadata": {
        "id": "7ddfad43-ec7d-401e-97cb-ad512401140d"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T171laWKpT2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "T171laWKpT2S",
        "outputId": "7335f4db-d4b8-42d4-a775-60094ac0965e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78a1365b-4758-4572-9ef6-d420f6547342\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78a1365b-4758-4572-9ef6-d420f6547342\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b4fdad-abd9-49e6-bdcc-35e152c5cb66",
      "metadata": {
        "id": "a1b4fdad-abd9-49e6-bdcc-35e152c5cb66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "aita_subset = pd.read_csv('AITA_minorities_subset.csv')\n",
        "\n",
        "train_data = aita_subset[101:200]\n",
        "test_data = pd.read_csv('aita_subset_titles.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30de9d6f-f689-47c4-a7bf-322443de62c3",
      "metadata": {
        "id": "30de9d6f-f689-47c4-a7bf-322443de62c3"
      },
      "source": [
        "# STEP1: Summarisation and Key points extraction using Llama 3.1 Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b201573-d60f-4536-9bfb-04a189fd173e",
      "metadata": {
        "id": "9b201573-d60f-4536-9bfb-04a189fd173e"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9197820c-c62e-457b-9042-43410f1e20b2",
      "metadata": {
        "id": "9197820c-c62e-457b-9042-43410f1e20b2"
      },
      "outputs": [],
      "source": [
        "post_generator_llama = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cMyg2s6idIoG",
      "metadata": {
        "id": "cMyg2s6idIoG"
      },
      "source": [
        "# STEP 3 Post Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8w949z8TwFiR",
      "metadata": {
        "id": "8w949z8TwFiR"
      },
      "outputs": [],
      "source": [
        "# this length is selected to accomodate the average length aita post\n",
        "post_max_tokens = 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3563a427-3fe8-45c1-9214-f88b0ade1a32",
      "metadata": {
        "id": "3563a427-3fe8-45c1-9214-f88b0ade1a32"
      },
      "outputs": [],
      "source": [
        "post_gen_system_message = f\"\"\"\n",
        "You are a highly intelligent language model designed to generate posts for the \"Am I the Asshole?\" (AITA) subreddit. Your task is to read the title of an AITA post and generate an engaging post that aligns with the provided judgement.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Read the Title:\n",
        "Carefully read the title of the AITA post.\n",
        "\n",
        "2. Acknowledge the Required Judgement:\n",
        "Consider the judgement indicated (NTA - Not the Asshole, NAH - No Assholes Here, ESH - Everyone Sucks Here). Think about the key points you need to make to ensure the story aligns with the required judgement.\n",
        "\n",
        "3. Generate a Story by Following those Steps:\n",
        "\n",
        "Create an AITA post without using section titles. Include the following elements seamlessly in the narrative:\n",
        "\n",
        "Brief Background: Provide context for the story. Introduce the main characters and their relationships, and set the scene for the events that will unfold.\n",
        "\n",
        "The Incident: Describe the specific incident or series of events that led to the conflict. Be detailed and clear about what happened, who was involved, and what actions were taken.\n",
        "\n",
        "Friends' or Family's Opinion: Describe the opinions of friends or family members about the situation. Include differing viewpoints to provide a balanced perspective on the conflict.\n",
        "\n",
        "The Current Outcome: Detail the consequences of the incident. Explain how the people involved reacted, any changes in relationships, and any ongoing impact the conflict has had.\n",
        "\n",
        "Conclusion:\n",
        "Summarize the key points of the story and pose the question to the readers: \"Am I the Asshole for [OP ACTIONS]?\"\n",
        "\n",
        "4. Maintain Authenticity:\n",
        "Ensure that the story feels realistic and relatable. Use natural language and tone as if a real person is sharing their experience.\n",
        "\n",
        "5. Adhere to the Judgement:\n",
        "Ensure that the generated story logically leads to the required judgment (e.g., if the judgment is NTA, the story should clearly indicate why the poster might be considered not the asshole).\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restricting length to avoid using lengthy posts into the prompt hence to many tokens\n",
        "# but also avoiding passing too short example to not prompt the model for short story generation\n",
        "percentile_75 = int(train_data['word_count'].quantile(0.75))\n",
        "percentile_25 = int(train_data['word_count'].quantile(0.25))\n",
        "train_data= train_data[train_data['word_count'] < percentile_75]\n",
        "train_data= train_data[train_data['word_count'] > percentile_25]\n",
        "train_data = train_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "aWPlDqBMID1f"
      },
      "id": "aWPlDqBMID1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tHMEzd6NrOM8",
      "metadata": {
        "id": "tHMEzd6NrOM8"
      },
      "outputs": [],
      "source": [
        "train_data['few_shot_input'] = 'Judgement: ' + train_data['label'] + ', Title: ' + train_data['title']\n",
        "train_data['few_shot_output'] = train_data['body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M6JU24CKrK4B",
      "metadata": {
        "id": "M6JU24CKrK4B"
      },
      "outputs": [],
      "source": [
        "def create_example(row):\n",
        "  one_shot_data = row\n",
        "  one_shot = []\n",
        "  for os_index, os_row in one_shot_data.iterrows():\n",
        "    one_shot.append({\"role\": \"user\", \"content\": os_row['few_shot_input']})\n",
        "    one_shot.append({\"role\": \"assistant\", \"content\": os_row['few_shot_output']})\n",
        "\n",
        "  return one_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZHwOLWgMsWZI",
      "metadata": {
        "id": "ZHwOLWgMsWZI"
      },
      "outputs": [],
      "source": [
        "test_data['gen_post_prompt'] = \"Judgement: \" + test_data['label'] + \", Title: \" + test_data['generated_titles']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I5DC5AHcsIi7",
      "metadata": {
        "id": "I5DC5AHcsIi7"
      },
      "outputs": [],
      "source": [
        "# this function will sample a random story with the desired title to use as one shot, to avoid skewing all the sotries into the same direction\n",
        "def format_post_gen_input(row):\n",
        "  if row['label'] == 'YTA':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'YTA'].sample(1))\n",
        "  elif row['label'] == 'NAH':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'NAH'].sample(1))\n",
        "  elif row['label'] == 'ESH':\n",
        "    one_shot = create_example(train_data[train_data['label'] == 'ESH'].sample(1))\n",
        "\n",
        "  system_message = [{\"role\": \"system\", \"content\": post_gen_system_message}]\n",
        "  user_message = [{\"role\": \"user\", \"content\": row['gen_post_prompt']}]\n",
        "  return system_message + one_shot + user_message\n",
        "\n",
        "\n",
        "test_data.loc[:, 'get_post_input'] = test_data.apply(format_post_gen_input, axis=1)\n",
        "#pprint(test_data.loc[:1, 'get_post_input'].tolist(), sort_dicts=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Sy0ZkToyjan",
      "metadata": {
        "id": "9Sy0ZkToyjan"
      },
      "source": [
        "## Clearing GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xIYMo0mEw-ht",
      "metadata": {
        "id": "xIYMo0mEw-ht"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# Run garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear CUDA memory\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Snm0LCRNuD23",
      "metadata": {
        "id": "Snm0LCRNuD23"
      },
      "source": [
        "## Generating post with Llama 3.1 Instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dymIo1YyKIQe",
      "metadata": {
        "id": "dymIo1YyKIQe"
      },
      "outputs": [],
      "source": [
        "def generate_post(pipe, inputs):\n",
        "  \"\"\"\n",
        "  :param pipe: text-generation pipeline\n",
        "  :param model_folder_path: list of messages\n",
        "  :return: list\n",
        "  \"\"\"\n",
        "  assistant_outputs = []\n",
        "\n",
        "  terminators = [\n",
        "      pipe.tokenizer.eos_token_id,\n",
        "      pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "\n",
        "  for out in tqdm(pipe(\n",
        "      inputs,\n",
        "      max_new_tokens=post_max_tokens,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.8\n",
        "  )):\n",
        "    assistant_outputs.append(out[0][\"generated_text\"][-1]['content'].strip())\n",
        "\n",
        "  return assistant_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zbyijZJWuMkq",
      "metadata": {
        "id": "zbyijZJWuMkq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "start_time = time.time()\n",
        "llama_gen_post = generate_post(post_generator_llama, test_data['get_post_input'].tolist())\n",
        "print(f'Time: {int(time.time() - start_time)} seconds')\n",
        "\n",
        "print(*llama_gen_post[:2], sep = \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*llama_gen_post[:1], sep = \"\\n\\n\")"
      ],
      "metadata": {
        "id": "fFgwYzDcT4rD"
      },
      "id": "fFgwYzDcT4rD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-s5ClvS6zyZr",
      "metadata": {
        "id": "-s5ClvS6zyZr"
      },
      "outputs": [],
      "source": [
        "test_data['llama_gen_post'] = llama_gen_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AwgUfWdTz-yd",
      "metadata": {
        "id": "AwgUfWdTz-yd"
      },
      "outputs": [],
      "source": [
        "test_data.to_csv('aita_subset_llama_posts.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5WA743RBRkH",
      "metadata": {
        "id": "y5WA743RBRkH"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"aita_subset_llama_posts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbIoZ4GxVfIe"
      },
      "id": "VbIoZ4GxVfIe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}